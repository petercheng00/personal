#+HUGO_BASE_DIR: ./

# These macros save on typing for linking to external files. Unfortunately the macros can't go inside the brackets, so we define the entire bracket syntax as a macro
#+MACRO: external_link [[https://raw.githubusercontent.com/petercheng00/personal/master/website/v2/petercheng/external_files/$1][$2]]
#+MACRO: external_image [[https://raw.githubusercontent.com/petercheng00/personal/master/website/v2/petercheng/external_files/$1]]

* Pages
  :PROPERTIES:
  :EXPORT_HUGO_SECTION: ./
  :END:
** About
   :PROPERTIES:
   :EXPORT_FILE_NAME: about
   :EXPORT_HUGO_TYPE: about
   :END:
   I'm a computer vision engineer based in the San Francisco Bay Area. I'm currently at Matterport, where I've worked on a variety of topics including image processing, point cloud alignment, surface reconstruction, and semantic segmentation. Previously, I worked at Amazon Lab126, and developed machine learning approaches for gesture recognition as well as analytics tools for face tracking. I received my B.S. and M.S. at UC Berkeley, where I was a member of the Vision and Image Processing Lab.

   -----

   {{{external_link(petercheng_resume.pdf, Click here for a pdf version of my resume)}}}
    #+INCLUDE: "../../../resume/resume.org" :lines "35-"

* Posts
  :PROPERTIES:
  :EXPORT_HUGO_SECTION: posts
  :END:
** Graph Cuts and Alpha Expansion
   :PROPERTIES:
   :EXPORT_FILE_NAME: graph_cuts
   :EXPORT_DATE: 2019-07-09
   :END:
   This post will cover the basic minimum-cut problem, explore its applicability to image-processing tasks, and show how the basic formulation and solution can be extended to solve some seemingly more-complicated problems.

*** Graph Structure
    A graph is a structure that consists of nodes and edges, where every edge forms a connection between two nodes. In the context of this post, each edge also has a weight, which represents some measure of how strongly its nodes should be held together.

// diagram of basic graph here

    A graph cut is a way of splitting the graph into 2 disconnected sub-graphs, by cutting (removing) certain edges. A minimum graph cut is the graph cut that minimizes the sum of the weights of the deleted edges. In the common case (which we will follow), we also must split the graph such that 2 specified nodes, s and t, end up separated.

// diagrams here

To sum things up, our basic graph cut problem takes place on a graph, where one node is labeled s, and another node is labeled t. The objective is to cut some number of edges such that there is no longer any way to traverse between s and t, and we want to minimize the total weight of cut edges.

// side note about constraints
    For the common min-cut setup that we will be looking at, there are some additional constraints.
    * All edges are undirected and have a non-negative weight or cost, corresponding to the cost incurred by breaking that edge
    * Nodes do not have a weight or cost.
    * All nodes should be connected - that is, it should be possible to reach any node from any other node by following edges.
    Frequently, a problem that doesn't initially satisfy these constraints can be made to do so simply by shifting weights around, adding a weight offset to each edge, or by adding new zero-weight or infinite-weight edges.


*** Solving Minimum Cut
    The min-cut problem has a dual (a different problem with identical solution), known as the max-flow problem, and max-flow is easier to think about and to solve. The max-flow problem re-imagines the graph as a series of pipes, with the objective of carrying fluid from node s to node t (usually referred to as the source node and the sink node respectively). Each edge's weight represents its capacity (maximum flow rate), and we wish to find the maximum flow that can occur out of the source and in to the sink, keeping in mind that as in real life, flow must be conserved.

// examples of max flow

    To solve max-flow, a simple intuitive algorithm is just to find a path from source to sink, and send the maximum flow through that path. Then, reduce the edge weights along that path by that flow, and repeat the process until no path with nonzero flow can be found. More details can be found at wikipedia, along with a whole host of other algorithms. The key takeaway however, is that maximum flow can be solved efficiently, in polynomial time.

    So, back to min-cut. Why is the min-cut solution the same as the max-flow solution? Max-flow and min-cut have the same solution because a min-cut must interrupt all the flow of fluid between the source and the sink. For max-flow there is some parallel set of edges that are fully saturated (otherwise we could still increase the flow), and act as a bottleneck for the flow. By definition then, that bottleneck is the cheapest place to cut edges and sever the flow.

*** Min-cut on Images
    We can translate a standard image into a graph by simply treating each pixel as a node, with edges between adjacent pixels. However, in most cases we won't have any 2 special pixels that we want to represent by source and sink, so we'll need a slighly more clever formulation to represent most problems. Considering the common scenario where we want to label each pixel of an image as either l_0 or l_1 (e.g. for foreground vs background segmentation), we arbitrarily create a source node representing l_0, and a sink node representing l_1. The l_0 and l_1 nodes then each get an edge to every single pixel, with edge weight representing the cost of NOT assigning that pixel to that label. We can also add edges between any pair of pixels, representing the cost of those 2 pixels having different labels. By definition, the min-cut of this graph will cut edges such that every pixel is connected either the l_0 node or the l_1 node, producing the lowest-cost labeling.




   here is some text
   \begin{equation}
   \label{eq:1}
   C = W\log_{2} (1+\mathrm{SNR})
   \end{equation}
   #+html: <font size="3" color="red">inline html</font>
   #+BEGIN_EXPORT html
   <font size="3" color="red">also html</font>

<div id="container"></div>
<script>
    // Let's first initialize sigma:
    var s = new sigma('container');

    // Then, let's add some data to display:
    s.graph.addNode({
      // Main attributes:
      id: 'n0',
      label: 'Hello',
      // Display attributes:
      x: 0,
      y: 0,
      size: 1,
      color: '#f00'
    }).addNode({
      // Main attributes:
      id: 'n1',
      label: 'World !',
      // Display attributes:
      x: 1,
      y: 1,
      size: 1,
      color: '#00f'
    }).addEdge({
      id: 'e0',
      // Reference extremities:
      source: 'n0',
      target: 'n1'
    });

    // Finally, let's ask our sigma instance to refresh:
    s.refresh();
  </script>

   #+END_EXPORT

** Serial Access for R8000/AC3200 (and other) Routers
   :PROPERTIES:
   :EXPORT_FILE_NAME: serial_router
   :EXPORT_DATE: 2019-07-04
   :END:
   {{{external_image(serial_router/router5.jpg)}}}
   So you bricked your router. Or maybe you just want a more convenient way to manage and monitor firmware upgrades (wiping settings via command is a lot more pleasant than holding down power buttons). Either way, adding serial access is pretty easy for many routers. I first did this a couple years ago, but I had to do it again recently, so I documented the process here for my current router (Netgear R8000/AC3200). I've also since discovered that there are pretty good instructions on the [[https://wiki.dd-wrt.com/wiki/index.php/Serial_Recovery][dd-wrt wiki]] and [[https://www.myopenrouter.com/article/how-set-serial-console-netgear-r8000][myopenrouter]] as well.
*** Tools
    The main thing you need is a setup that has usb on one end (for the computer), and standard serial pins (at least RX, TX, ground) on the other end. *Important:* the serial side needs to be at 3.3v, and usb operates at 5v, so make sure you have a level shifter in there somewhere. I believe there's some cables that have this all in one package, but I ended up using [[https://smile.amazon.com/OSEPP-Breakout-Board-Arduino-Compatible/dp/B007JBSSGQ][this breakout board]] which I purchased from Fry's. Anything that mentions USB to TTL, and 3.3V should work fine though. If you use a board like this you'll also need some wires and possibly a soldering iron (though tape or extra hands work just fine for a temporary unbricking setup).
*** Getting to the pins (R8000 specific)
    1. Remove the torx screws on the bottom and back, including {{{external_link(serial_router/router1.jpg, the one hidden under the bottom label)}}} (no turning back after the label is broken, if you care about warranty!)
    2. Flip the router over, remove the bottom cover, and detach the antennas (6 colored wires), which {{{external_link(serial_router/router2.jpg, should look something like this)}}}.
    3. There's still a ribbon cable attaching the main board to the rest of the router, but it's long enough that the board can be flipped over without disconnecting it, {{{external_link(serial_router/router3.jpg, like this)}}}. The serial pins are now accessible (top left in the prior image).
*** Pin layout
    {{{external_image(serial_router/router4.jpg)}}}
    In the above image, the pin with the red wire attached is RX, orange is TX, and yellow is ground. The 4th pin is not needed here. If you are using a breakout board like me, keep in mind that RX on the router should go to TX on the board, and vice versa. The image at the top of this post shows my final setup, complete with drilled hole for semi-permanent access (note the red and orange wires swapping near the breakout board).
*** Computer stuff
    On the computer end, any serial program like PuTTY or minicom will work. Find and select the usb device via something like device manager or dmesg, set baud rate to 115200, and everything else should pretty much be defaults. With everything connected, you should be able to see a stream of text output whenever the router boots. If you don't, you can verify your setup by disconnecting from the router and shorting between RX and TX, and making sure any typed text is echoed back.
*** Commands
    To get to a command prompt, reboot the router and mash ~Ctrl-C~ a bunch as it starts up.
    Pretty much the only command I use is ~nvram erase~, which resets router settings, and has generally resolved any boot issues I've encountered. You can also apply and transfer new firmware over telnet for more serious problems, and do a whole bunch of other things, but I'll leave those for other sites to cover, at least until I cause more problems and need to figure those things out for myself.

** Hungarian Matching Demo
   :PROPERTIES:
   :EXPORT_FILE_NAME: hungarian_matching
   :EXPORT_DATE: 2019-07-03
   :END:
   Back in 2013, as a class project, we built a javascript demo of the hungarian algorithm. The basic idea is that it's a polynomial-time method to obtain the optimal matching between 2 sets of objects (e.g. matching people to resources), where every pairing has some cost (or reward) associated with it. I had never used javascript before this project, and I never used it again afterwards, so no idea if the code itself is any good, but it was a fun project.

<iframe width=1000 height=700 src=../../files/graphVisualizer/graphVisualizer.html></iframe>

** Building Meshlab from Source in Ubuntu
   :PROPERTIES:
   :EXPORT_FILE_NAME: meshlab-build
   :EXPORT_DATE: 2018-06-16
   :END:
   Every time I build Meshlab, it's always a little more work than it really should be. So here's my notes from my most recent build (June 2018, Ubuntu 18.04)

   Clone the repositories (This is for building master, switch to a release branch/tag if you prefer)
   #+BEGIN_SRC sh
   git clone git@github.com:cnr-isti-vclab/meshlab.git
   git clone git@github.com:cnr-isti-vclab/vcglib.git -b devel
   #+END_SRC
   Install dependencies (You may need other dependencies, these are just the ones that I needed at this point in time)
   #+BEGIN_SRC sh
   sudo apt install qt5-qmake qtscript5-dev libqt5xmlpatterns5-dev libqt5widgets5 libqt5gui5 libqt5network5 libqt5core5a libdouble-conversion1 libxcb-xinerama0
   #+END_SRC
   Build external plugins
   #+BEGIN_SRC sh
   cd meshlab/src/external
   qmake -qt=5 external.pro
   make -j6
   #+END_SRC
   Build common project
   #+BEGIN_SRC sh
   cd ../common
   qmake -qt=5 common.pro
   make -j6
   #+END_SRC
   At this point I encountered an error about =ReadHeader=. The following GitHub issue contains a fix, and I've pasted the patch below
   https://github.com/cnr-isti-vclab/meshlab/issues/188
   #+BEGIN_SRC diff
   diff -ru vcglib/wrap/io_trimesh/import_nvm.h vcglib/wrap/io_trimesh/import_nvm.h
   --- a/vcglib/wrap/io_trimesh/import_nvm.h	2016-12-29 12:54:58.000000000 +0300
   +++ b/vcglib/wrap/io_trimesh/import_nvm.h	2017-12-28 12:20:14.591670159 +0300
   @@ -85,15 +85,6 @@
   return true;
   }

   -static bool ReadHeader(const char * filename,unsigned int &/*num_cams*/, unsigned int &/*num_points*/){
   -    FILE *fp = fopen(filename, "r");
   -    if(!fp) return false;
   -    ReadHeader(fp);
   -    fclose(fp);
   -    return true;
   -}
   -
   -
   static int Open( OpenMeshType &m, std::vector<Shot<ScalarType> >  & shots,
   std::vector<std::string > & image_filenames,
   const char * filename, CallBackPos *cb=0)
   diff -ru vcglib/wrap/io_trimesh/import_out.h vcglib/wrap/io_trimesh/import_out.h
   --- a/vcglib/wrap/io_trimesh/import_out.h	2016-12-29 12:54:58.000000000 +0300
   +++ b/vcglib/wrap/io_trimesh/import_out.h	2017-12-28 12:20:48.434017234 +0300
   @@ -85,15 +85,6 @@
   return true;
   }

   -static bool ReadHeader(const char * filename,unsigned int &/*num_cams*/, unsigned int &/*num_points*/){
   -    FILE *fp = fopen(filename, "r");
   -    if(!fp) return false;
   -    ReadHeader(fp);
   -    fclose(fp);
   -    return true;
   -}
   -
   -
   static int Open( OpenMeshType &m, std::vector<Shot<ScalarType> >  & shots,
   std::vector<std::string > & image_filenames,
   const char * filename,const char * filename_images, CallBackPos *cb=0)
   #+END_SRC
   Build meshlab itself
   #+BEGIN_SRC sh
   cd ..
   qmake -qt=5 meshlab_full.pro
   make -j6
   #+END_SRC
   I hit an error about missing libraries - the following fixed it for me
   #+BEGIN_SRC sh
   cp external/lib/linux/* external/lib/linux-g++
   #+END_SRC
   If everything worked, the meshlab binary will be at src/distrib/meshlab
** Dual-booting Ubuntu 18.04 with macOS (including full disk encryption)
   :PROPERTIES:
   :EXPORT_FILE_NAME: ubuntu-on-macbook-pro
   :EXPORT_DATE: 2018-06-08
   :END:
*** Introduction
     I've been running Ubuntu on Macbook Pros for a couple years now, and while the ease of installation, driver support, and general stability has greatly improved in recent years, it can be difficult to find up-to-date guides. I've recently set up a mid-2015 macbook pro dual booting macOS with Ubuntu 18.04, so I figured I'd document my steps. First some overall notes and warnings, then simple instructions for a non-encrypted install, followed by slightly longer instructions for an encrypted install.

*** Notes and Warnings
     * I've heard that support for the newer touchbar-equipped macbook pros is not great. I have not tried those, but I've used a mid-2014, as well as 2 variants of mid-2015 macbook pros long-term, on 14.04, 16.04, and 18.04.
     * If you get your disk into any terrible state, macbooks come with pretty great recovery options. Command-R will boot into a recovery partition, and even if that's lost, Option-R will get you into an internet-recovery mode.
     * If you want to remove Ubuntu, and find that grub is still hanging around (or somehow end up with an extraneous grub), run the following from macOS.
       #+BEGIN_SRC sh
       mkdir mnt
       sudo mount -t msdos /dev/disk0s1 mnt
       sudo rm -rf mnt/EFI/ubuntu
       #+END_SRC
     * System upgrades of either macOS or ubuntu may cause refind to lose priority and make it more difficult to dual-boot. If that happens, you can run refind-mkdefault, which is available in the mac download as explained below, or from ~sudo apt install refind~. For more information see this [[https://www.rodsbooks.com/refind/bootcoup.html][handy guide]] from the refind website.

*** Create Ubuntu bootable USB
     Instructions for [[https://tutorials.ubuntu.com/tutorial/tutorial-create-a-usb-stick-on-macos#0][macOS]], [[https://tutorials.ubuntu.com/tutorial/tutorial-create-a-usb-stick-on-ubuntu#0][Ubuntu]], [[https://tutorials.ubuntu.com/tutorial/tutorial-create-a-usb-stick-on-windows#0][Windows]]

*** Prepare macOS
     First thing we'll need to do is reduce your macOS partition size in order to make some space for Ubuntu. This should be fairly straightforward using macOS's Disk Utility applicaton.

     Next, install rEFInd, which is available [[http://www.rodsbooks.com/refind][here]], and run the refind-install binary. Most likely you'll see an error message about System Integrity Protection being enabled. As the error message suggests, we can either install from the recovery partition, or temporarily disable SIP. To get into recovery mode, hold command + r while booting, and from there a terminal can be accessed via the Utilities menu. You can try running refind-install from recovery mode, but I had no luck with that, and just got the same error. So instead, I ran =csrutil disable= to disable SIP. After a reboot (back to non-recovery mode, because it's faster), refind-install should work. You can then re-enter recovery mode to run =csrutil enable=. After this process, you should now see the refind menu whenever you boot. You'll be able to choose between macOS and any other operating systems you load, as well boot from external drives.

*** Install Ubuntu 18.04 (no encryption)
     After booting from the Ubuntu bootable USB, you can either install straightaway, or do it from within the "try ubuntu" environment. Either way, the only important step is to select "Something else" on the menu that asks how/where to install Ubuntu. You should see the empty space on your disk that you freed up from macOS, and be able to add partitions. This is my configuration:
     * Boot partition, 500 MB, ext4, mounted at /boot (sda4 for me)
     * Root partition, remaining space, ext4, mounted at / (sda5 for me)
     * Bootloader installed to boot partition (sda4 for me)
     You could optionally add a swap partition, but Ubuntu 18.04 now supports swap files

     Everything should be good to go from here. As a side-note the installer crashed for me apparently because I had another copy of grub hanging out on my /sda from some earlier tests. Deleting it per the notes above, and then retrying worked for me.

*** Install Ubuntu 18.04 with full disk encryption
     After going through the above process, I discovered that Ubuntu 18.04 no longer supports homedir encryption. Furthermore, while full disk encryption is an option in the installer, it requires wiping the entire physical disk. So that's not great either. Fortunately, I came across this well-written [[https://blog.jayway.com/2015/11/22/ubuntu-full-disk-encrypted-macosx/][blog post]] that provides all the details to manually encrypt the Ubuntu partition before installing. As above, I chose to skip the swap partition steps, and otherwise followed it with only one issue.

     Strangely, I again had the installer repeatedly crash on me while "copying files". This time it was not due to any grub conflicts that I could find. As an unsatisfying workaround, I realized that it would only crash after I entered my account/login details. So I simply stayed on that screen until the activity led on my usb drive stopped flashing. I then continued forward, allowed it to crash, and then moved on with the post-installation instructions, and so far things are working.
** Publishing a Website from Emacs and Hugo
   :PROPERTIES:
   :EXPORT_FILE_NAME: website-v2-setup
   :EXPORT_DATE: 2018-06-04
   :END:
*** Introduction
   After 5 years, it's time to give the site a bit of a refresh, now with fewer images and more words. Previously I used bootstrap plus a bit of manual editing. This time I'll be using a pipeline of Emacs org-mode -> ox-hugo -> hugo -> nearlyfreespeech.net. This post will self-document my steps to get all that up and running. The last time I did any web-related things was over 5 years ago, and I wasn't an expert then, so these steps should be taken with a grain of salt.
*** Hugo Setup
    #+BEGIN_SRC sh
    sudo snap install hugo
    mkdir petercheng && cd petercheng
    hugo new site petercheng
    #+END_SRC
    Emacs init:
    #+BEGIN_SRC lisp
    (use-package ox-hugo
        :ensure t
        :after ox)
    #+END_SRC
    Set up a theme (I'm using the [[https://themes.gohugo.io/hyde-hyde/][hyde-hyde]] theme)
    #+BEGIN_SRC sh
    git submodule add https://github.com/htr3n/hyde-hyde.git themes/hyde-hyde
    #+END_SRC
*** config.toml
    For my intended setup, there are only 2 files I'll be working with. The first one is ~config.toml~, which stores global hugo settings, as well as parameters for my chosen theme. I'm not really sure how to find all the toggle-able parameters for a given theme besides digging through the theme code or looking at example sites.

    As an early example of why I'm using org mode, I can directly insert a live copy of my ~config.toml~ file below, simply by including the line:

    ~#+INCLUDE: "config.toml" src ini~
    #+INCLUDE: "config.toml" src ini

    One early roadblock I hit was that hyde-hyde uses highlight.js for syntax highlighting, which does not contain ~emacs-lisp~ as a language option, unlike org-mode and chroma (hugo's default syntax highlighter). I'm currently using ~lisp~ as a compromise, and it took me a while to realize that highlightjslanguages needed to be set to include non-default languages in highlight.js. If an unsupported (or empty!) language is passed to highlight.js, at least with hyde-hyde, it results in poorly formatted output, which led to much confusion for a while.

*** petercheng.org
    The other file I need to create is the org file that generates all this content, on every page, following ox-hugo's single-page architecture. In normal Hugo, individual pages written in markdown (or now in org-mode) are placed inside the ~content~ directory inside the project root. With ox-hugo, a single org-mode file can be used to generate all pages, posts, and any other content. This has some advantages in allowing usage of org-mode functionality, as well as re-use of content or property settings across pages.

    There's a number of hugo properties that can be set within the file, but the only required one is ~HUGO_BASE_DIR~, which specifies the root directory of the hugo website, relative to the org file.
    #+BEGIN_SRC sh
    #+HUGO_BASE_DIR: ./
    #+END_SRC

    Afterwards, I have 2 top-level sections in my org file, ~Pages~, and ~Posts~. Any properties set under a section will be applied to subsections, so I have the following properties set for each, to place pages at the top level of my exported files, and posts within a subdirectory.
    #+BEGIN_SRC sh
    * Pages
        :PROPERTIES:
        :EXPORT_HUGO_SECTION: ./
        :END:
    * Posts
        :PROPERTIES:
        :EXPORT_HUGO_SECTION: posts
        :END:
    #+END_SRC
    I can then create pages or posts by creating subsections within the relevant section. The ~EXPORT_FILE_NAME~ property is required to be set for each, which determines the exported filename. Here's an example of the properties setting for this current post.
    #+BEGIN_SRC markdown
    ** Publishing a Website from Emacs and Hugo
        :PROPERTIES:
        :EXPORT_FILE_NAME: website-v2-setup
        :EXPORT_DATE: 2018-06-04
        :END:
    #+END_SRC

*** Exporting
    Ox-hugo adds a new export option to the org-mode export menu. ~(C-c C-e)~ by default. There's a few options for exporting, but currently I find it simplest just to always export all content, with ~(C-c C-e H A)~. One setting I've seen used a lot is ~#+HUGO_AUTO_SET_LASTMOD: t~, and that doesn't play nicely if always updating all files. But I don't feel a need to track and update dates on every edit.

    After exporting, markdown files should be created in the content directory, and hugo will auto-reload pages if already running (to start hugo, run ~hugo server~ from the base directory).

*** Getting Online
    There are some fancy options for deploying, such as [[https://www.penwatch.net/cms/get_started_plain_blog/][this guide]], which demonstrates hugo publishing on a remote server, triggered by git post-receive. For the time being I'm going to keep thing simple, and simply use a script to generate a static site, which I'll keep synced up via rsync. A final example of showing a live code view of my publishing script:
    #+INCLUDE: "publish.sh" src bash


