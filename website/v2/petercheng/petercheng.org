#+HUGO_BASE_DIR: ./

# These macros save on typing for linking to external files. Unfortunately the macros can't go inside the brackets, so we define the entire bracket syntax as a macro
#+MACRO: external_link [[https://raw.githubusercontent.com/petercheng00/personal/master/website/v2/petercheng/external_files/$1][$2]]
#+MACRO: external_image [[https://raw.githubusercontent.com/petercheng00/personal/master/website/v2/petercheng/external_files/$1][https://raw.githubusercontent.com/petercheng00/personal/master/website/v2/petercheng/external_files/$1]]
#+MACRO: other_repo_link [[https://raw.githubusercontent.com/petercheng00/$1][$2]]
#+MACRO: other_repo_image [[https://raw.githubusercontent.com/petercheng00/$1]]
* Pages
  :PROPERTIES:
  :EXPORT_HUGO_SECTION: ./
  :END:
** About
   :PROPERTIES:
   :EXPORT_FILE_NAME: about
   :EXPORT_HUGO_TYPE: about
   :END:
   I'm a computer vision engineer in the San Francisco Bay Area. I'm currently at Iron Ox, where we're optimizing greenhouse farming to make quality produce more accessible. Previously, I spent 5 years at Matterport, making 3d technologies for capturing and visualizing 3d spaces available to everyday users. Before that, I worked at Amazon Lab126, focusing on 3d interfaces and interactions with mobile devices. I received my B.S. and M.S. at UC Berkeley, where I was a member of the Vision and Image Processing Lab.

   -----

   {{{other_repo_link(personal/master/resume/resume.pdf, Click here for a pdf version of my resume)}}}
   # We don't want the header of the resume org file to show up
   #+INCLUDE: "../../../resume/resume.org" :lines "38-"

* Posts
  :PROPERTIES:
  :EXPORT_HUGO_SECTION: posts
  :END:
** Ethernet over Coaxial Cable via MoCA
:PROPERTIES:
:EXPORT_FILE_NAME: moca
:EXPORT_DATE: 2021-01-12
:END:
I recently used MoCA adapters to expand wifi coverage in a large house. MOCA allows me to pass an ethernet connection from my router into the house's coaxial cabling, and then place wireless access points at other coaxial terminals. In my mind, MOCA works better and is less well-known than mesh wifi and powerline, both of which I originally considered, so this post covers some basics on how it works.
*** Why MoCA
Here's the few approaches I considered.
| Name      | Flexibility                                  | Speed                                                                  | Cost                                                        |
|-----------+----------------------------------------------+------------------------------------------------------------------------+-------------------------------------------------------------|
| Mesh Wifi | Can be placed anywhere                       | Theoretically very high, in practice limited by wifi congestion        | $300-$500 for cheaper setups, more for higher performance   |
| Ethernet  | Most likely will have to run cables yourself | Maximum                                                                | Cables are cheap, running cables might not be               |
| Powerline | Power outlets are usually in good places     | Very high between outlets on a clean shared circuit, less so otherwise | ~$50 for a pair of adapters                                 |
| MoCA      | Many houses are wired with coaxial cable     | Moca 2.5 provides 2.5 Gbps shared between all adapters                 | ~$60 per adapter. Possibly minor costs for filter, splitter |

Running ethernet through the house would have been a big project, and powerline adapters were tested and found to provide poor connection to the parts of the house that most needed additional wifi coverage. Given the prevalence of coaxial ports in the house, MoCA was a good choice.
*** Required Components
 * At least 2 [[https://www.amazon.com/gp/product/B07XYDG7WN][MoCA adapters]]. One will be used to connect your internet source to the coaxial cabling, the others will connect clients or access points to the coaxial cabling. Multiple adapters can be used together, and will properly switch and share bandwidth. I'm using 3 MoCA 2.5 adapters from goCoax. Note that many adapters, such as the goCoax adapter, have 2 coax ports, and internally contain a splitter. If you buy an adapter with just 1 coax port, and you are using your coaxial cabling for other applications, you'll need additional splitters.
 * (Probably) [[https://www.amazon.com/gp/product/B00KO5KHSQ][A MoCA filter]]. This provides security by preventing your internet signals from leaving your house. This also strengthens the signals between your adapters, by reflecting the outgoing MoCA data back into the house. Some houses may already have a MoCA filter placed by the ISP, but if you're not sure, an extra one doesn't hurt.
 * (Maybe) Coaxial and ethernet cables. Depending on your setup, you may need additional ethernet and coaxial cables. Any standard cables will work.
 * (Maybe) [[https://www.amazon.com/gp/product/B07PRYWD81][A coax splitter]]. Most houses will already have a coax splitter. I replaced mine for 2 reasons. First, MoCA signals use 500-1500 MHz, so a splitter that accommodates that range will theoretically perform better. Second, my house had a 3-way splitter, with 3 other coax cables left unconnected. This 6-way splitter lets me have all 6 connected at once.
 * (Maybe) [[https://www.amazon.com/gp/product/B06XHYNMVZ][Weather seals]]. My cable box is outside, so the new coax splitter required some new weather seals for better waterproofing.
 * (Maybe) [[https://www.amazon.com/gp/product/B07C53S78W][Coax installation tools]]. Some of the coax cables I wanted to use were unterminated, so I needed to add plugs. To make matters more complicated, my coax cables are RG6Q, which is less common and wider. Tools for RG59 and RG6 are cheaper, but this set of tools supports RG6Q and worked great for me.
*** Installation
Cable box, before:
{{{external_image(moca/cables_before.jpg)}}}
Cable box, after:
{{{external_image(moca/cables_after.jpg)}}}
1. Install your MoCA filter. This should be connected to the cable that runs between your house and the external world. If your house only has one splitter, it'll be the cable that's on the solitary side of the splitter.
2. Replace your splitter, if applicable.
3. At each MoCA adapter location, unplug any devices currently using the coaxial cabling. If using the goCoax adapters, connect those devices to the "TV" port of the adapter. Then, connect the "MoCA" port of the adapter to the wall. If using an adapter with only one port, you'll need a 2-way splitter, with the single side plugged into the wall.
4. If your MoCA adapter supports it, enable encryption. You should be safe with a MoCA filter, but the extra security has no overhead.
5. Connect your router to an adapter via ethernet, connect your client network devices to the other adapters.
*** Conclusion
Previously, our single wireless router was barely reaching parts of the house, even with a 2.4Ghz network, and when multiple client devices were active, the wireless speeds would plumment. Now, I have 2 additional Unifi wireless APs connected via MoCA, and 5Ghz signals are available to the entire house, closely matching ISP speeds. The costs ended up being higher than I anticipated, due to the unexpected requirements of changing the splitter and connecting unterminated cables. That said, the performance/cost is likely still better than a mesh wifi setup, and ability to scale and upgrade wifi components in the future is better as well.

** Ergonomic Split Keyboard Build (Iris Rev 4)
:PROPERTIES:
:EXPORT_FILE_NAME: iris_build
:EXPORT_DATE: 2020-09-12
:END:
{{{external_image(iris_build/finished.jpg)}}}
*** Keyboard Choice
I'm making the move to a split ergonomic keyboard as a reponse to occasional wrist pain when working at a computer. Splitting the keyboard allows for much more freedom in wrist positions and angles, especially if the board is tented.

Some split boards simply take a standard keyboard layout and split it in half (e.g. Mistel Barocco, Quefrency). Other boards instead use a columnar layout, usually with thumb keys as well. The Ergodox is likely the most well-known example in this family. As a frequent thumb-alt user due to Emacs, the thumb clusters made a lot of sense to me, so I looked in that direction.

A common complaint of the ergodox is that its thumb keys are a stretch for many users to reach. [[https://jhelvy.shinyapps.io/splitkbcompare/][This website]] is an excellent way of comparing different keyboard layouts, and by printing out a few overlays, I found the Iris to be a good fit. Two other very strong reasons for me to pick the Iris were that it's a very popular board, with plenty of support and hardware availability, and also that its latest revision comes with all the components already soldered on (except switches), making it simple to build.
*** Components
The key decisions to make are which switches and which keycaps to use. I went with Aqua Zilents, as they seem highly recommended for tactility without too much noise. I also picked up some Tribosys 3203 lube to further reduce switch noise, and some Mill-Max 0305 sockets so switches could be replaced without (de)soldering. For the keycaps, I chose MDA Big Bang 2.0, which is one of the few keycap sets that is sculpted per-row in a manner appropriate for split keyboards.
{{{external_image(iris_build/components.jpg)}}}
The full list of parts:
 * MDA Big Bang 2.0 ortholinear keycaps
 * 65 x Aqua Zilent 62 g switches
 * Tribosys 3203 lube
 * 150 x Mill-Max sockets (not pictured above)
 * 1 x rotary encoder with knob from Keebio (2 are supported, but I couldn't think of a use for a 2nd one)
 * FR4 plates for Iris from Keebio
 * PCBs for Iris from Keebio
 * TRRS cable (used to link the 2 halves together)
 * USB-C cable (not pictured above)
*** The Build
[[https://docs.keeb.io/iris-rev3-build-guide/][A comprehensive build guide]] for the Iris is already available, but I learned a few things during the process that might be worth sharing.
 * While dedicated switch opening tools exist, I found tweezers work quite well, and significantly better than a single flat-head screwdriver.
   {{{external_image(iris_build/opening.jpg)}}}

 * For the lubing process, I followed [[https://www.youtube.com/watch?v=qSgPKPoFo2k][this guide from Taeha Types]].
   {{{external_image(iris_build/lubing.jpg)}}}

 * Mill-Max sockets are designed for the thinner pins on most switches. The Zilents though, have one pin that is wider. With some force, they can be made to fit, but after I snapped one pin, I discovered two things that help.
   {{{external_image(iris_build/pins.jpg)}}}
   A bit of sandpapering on the wide tips of the pins makes a large difference. It's hard to see in the image above, but the right edge of the wide pin on the right switch has been sanded off, making it easier to slide the Mill-Max socket on. I found that sanding just one side was enough.
   {{{external_image(iris_build/millmax.jpg)}}}
   Unless you want to spend a lot of time sanding, some amount of force is still usually needed. I found the most secure method to apply force without risk of bending the pins is to push down with thumbnails on the lip on opposite sides of the socket.

 * While Mill-Max sockets are supposed to be soldered in, I tested out the keyboard before soldering, and found that all the switches are working fine. Soldering is definitely more secure, especially since without solder, the PCB is just held up by friction. I don't have the best soldering setup though, so I plan to continue unsoldered unless any problems arise.

*** Initial Impressions
The biggest surprise for me was that my thumb position is actually different on the columnar layout vs. on a normal staggered keyboard. I had intended to use the middle thumb key for space bar, but I find the farthest one a better fit. The other surprise is that columnar layout is quite easy to get used to. After only 1 day, my typing is basically back up to speed for a-z keys. I'm still fiddling with the keymapping, and still need to figure out tenting hardware, but overall I'm quite happy typing on this board, and MDA + Zilents feel great to type on.
{{{external_image(iris_build/finished2.jpg)}}}


** Understanding Stock Options
:PROPERTIES:
:EXPORT_FILE_NAME: stock_options
:EXPORT_DATE: 2020-04-04
:END:
*Disclaimer: this post contains my current best understanding of topics, I am not an expert and make no guarantees*
*** The Basics
    A stock option gives the owner of the option the ability to purchase a given stock at a given purchase price. This process is known as "exercising" an option. Stock option grants are commonly given to startup employees, and contain 4 main pieces of information.
    1. How many options are being granted. E.g. a stock option grant of 10,000 options allows for the optional purchase of up to 10,000 shares.
    2. The strike price of the options. This is the price per share that the owner of the options must pay to purchase shares. This price stays fixed, even if the company's valuation changes over time.
    3. Vesting schedule. The stock options being granted do not enter into your possession (and thus cannot be used to purchase shares) until they vest. (Except in the case of early exercise)
    4. Option type. Options are either incentive stock options (ISOs), or non-qualified stock options (NSOs). ISOs can only be owned by employees, and employees leaving a company must exercise ISOs within 3 months or they are forfeited. NSOs do not have such restrictions. At some companies, ISOs can be converted to NSOs, which can allow departing employees to avoid the 3-month deadline. If exercised ISOs are sold at least 2 years after grant and 1 year after exercise, this counts as a qualifying disposition, which has some tax benefits over NSOs that will be discussed below.
*** Exercising Stock Options
    Once an option is vested, it can be exercised by paying the strike price. At private companies, shares cannot be sold for cash, so exercising is making the bet that the upfront cost of exercise (plus associated tax costs) is worth the future value of the shares once they can be liquidated (as well as the time and opportunity cost).
***** Early Exercise
    Some companies allow for early exercise - this allows an option to be exercised before it has vested (which confers all the usual results of owning a share). The direct cost of exercising does not change, but exercising early generally means exercising when a company has a lower valuation, which can reduce the associated tax costs covered in the next section. To receive these tax benefits, an 83(b) form must be submitted within 30 days of the early exercise event. Exercising early also means shares are owned earlier, and shares owned for longer may get favorable tax treatment when sold. If an employee leaves before their early-exercised options are vested, usually the company will purchase the unvested shares back at the original strike price.
*** Tax Implications
    Taxes are owed when an option is exercised, as well as when a share is sold. The former can be quite surprising to startup employees, as upon exercising, they may owe a large tax bill, despite not being able to sell their shares to help pay that bill. ISOs and NSOs are taxed differently at both events, as we'll cover next.
***** Taxes Owed when Exercising
     To calculate taxes owed upon exercising, the strike price must be compared against the fair market value (FMV) of the stock. For a private company, this is determined by its last 409A valuation, which usually occurs every 12 months, and also upon significant events such as fundraising. Note that when private company valuations are being discussed, these are often post-money valuations based on the prices VCs are paying. VCs are paying for preferred stock however, which is priced significantly higher than common stock. FMV is thus much lower than the numbers often mentioned around company valuations.

     ~FMV - [strike price]~ at the time of exercise is referred to as the bargain element, which is the value that matters for taxes. Note that this is why early exercise can be a significant cost-savings, as exercising immediately after an option grant means FMV may be equal to strike price, leading to zero bargain element. For NSOs, the bargain element is taxed as ordinary income. For ISOs the bargain element is not taxed at exercise time, but may be taxed at stock sale time in the event of a non-qualifying disposition (sold within 2 years of 1 grant or within 1 year of exercise). For ISOs that are not sold the same year they are exercised (i.e. all qualifying dispositions), the bargain element is subject only to AMT (alternative minimum tax).
***** AMT for ISOs
      AMT, as the name implies, is an alternative way of calculating taxes, and if AMT is higher than regular tax calculations, AMT must be paid instead. To roughly estimate AMT, first add the bargain element to other income, then subtract the AMT exclusion amount, and then multiply the result by the AMT tax rate. If the result of this computation is higher than the amount of taxes normally paid, then this should be factored as an extra cost of exercising options. 

      However, paying AMT due to ISO exercise also grants AMT credit, of the amount corresponding to the ISO exercise. In future years, if the regular tax owed is greater than AMT, AMT credit can be used to reduce the regular tax bill until reaching the AMT as a lower limit. AMT credit does not expire, so for people who don't normally pay AMT, tax costs for ISO exercising can be entirely recouped over time.

***** Taxes Owed when Selling Shares
     Stock sales are taxed as either short-term or long-term capital gains, applied to the difference between sell price and exercise price. Exercised NSOs simply follow the same rules as normal stock sales. Exercised ISOs on the other hand, must follow the rules for qualifying dispositions (held for 2 years after grant, 1 year after exercise) to receive long-term capital gains treatment. Otherwise, ISO sales that do not qualify are subject to short-term capital gains, and even worse, as discussed above, the bargain element also gets taxed as ordinary income.

** Literate Emacs Init with Org Babel
:PROPERTIES:
:EXPORT_FILE_NAME: emacs_init
:EXPORT_DATE: 2020-02-29
:END:
Over the past few years, my emacs init files have gotten a bit out of hand. My once-cleanly-categorized files have started to blend together, and there's a fair amount of dead code, as I've made many significant changes to my emacs tools (e.g. I've switched static analyzers and terminals basically each year).

*** Org Babel
Org-mode is a great way to mix inline code with general writing (and it's what this website is built with), so it's a natural fit for managing and documenting my init files as well. The basic idea is to write all init code inside an org file, where the lisp code to be executed is inlined inside code blocks like so:

#+BEGIN_SRC markdown
,* Here's a category heading
    Description of below code here
    ,#+BEGIN_SRC emacs-lisp
      (some-lisp-code)
    ,#+END_SRC emacs-lisp
    More description for more code here
    ,#+BEGIN_SRC emacs-lisp
      (some-more-lisp-code)
    ,#+END_SRC
#+END_SRC

Org-mode's Babel can then parse these files and extract the code blocks out into a nice clean source file for emacs to read natively. This process is called "tangling", and it has some runtime cost, but only needs to be run after file changes.

To get started with this conversion, I simply wrapped all of my lisp code inside a giant src block inside a new ~config.org~ file. Then, I simply set ~(org-babel-load-file "~/emacs/config.org")~ in ~.emacs~. With this done, it's been fairly straightforward to break my init code into more manageable chunks, as having everything back together in a single file makes it easier to get a high-level view of how things are organized, and also discover plenty of old and unused code.

*** Literate Config
With my config now being in org-mode, [[https://github.com/petercheng00/emacs/blob/master/config.org][it renders nicely on github]], making it easy to reference, which also forces me to keep things well-documented and organized.

*** Straight.el
The other major change I've made is a transition from the built-in ~package.el~ to ~straight.el~, though I'm still using ~use-package~ as a front-end. Straight.el already has [[https://github.com/raxod502/straight.el#tldr-1][a great list of pros/cons of why to use it]], but for me the chief benefit is reproducibility, and reducing conflicts when I sync my config across machines. There's also a nice feeling of cleanliness, where all the packaging infrastructure is now basically git, instead of the somewhat opaque MELPA installation process.

** Mixed-Precision Neural Network Training with APEX
   :PROPERTIES:
   :EXPORT_FILE_NAME: nvidia_apex
   :EXPORT_DATE: 2020-02-21
   :END:

   *TLDR: Just make these changes:*
   #+BEGIN_SRC python
   from apex import amp
   # add this after net and optimizer are defined:
   net, optimizer = amp.initialize(net, optimizer, opt_level='O1')
   # replace 'loss.backward()' with this:
   with amp.scale_loss(loss, optimizer) as scaled_loss:
       scaled_loss.backward()
   #+END_SRC

*** Background
   I have a Turing GPU, which contains hardware optimized for efficient FP16 (half-precision floating point) processing. This is useful because gpu memory is often a bottleneck in deep learning - doubling the size of a network or doubling batch size can have a sizable impact. It's been shown that reducing the precision of neural network operations often has minimal impact on performance, so switching to half-precision can in theory be a free upgrade. As an example, in a small test training session, at the default FP32, I have ~5 GB gpu memory being used. Training for 1 epoch takes 160 seconds, and results in a training loss of 0.02.

   In PyTorch, switching to half-precision is as simple as

   #+BEGIN_SRC python
   net.half()
   half_tensor = tensor.half() # cast to half_tensors as needed before inputting to network
   #+END_SRC

   And indeed, with these changes, memory usage is now ~3 GB.
   But...

   #+BEGIN_SRC sh
   Epoch [1/1], Step[10/255], Loss: nan
   Epoch [1/1], Step[20/255], Loss: nan
   ...
   #+END_SRC

   As it turns out, while the network itself may not need much precision, the training process does. In this case, some computation within our loss function or our optimizer is becoming numerically unstable, leading to divide-by-zeros. Some stack overflow searching suggests that modifying the epsilon values used by optimizers and batch norm layers could help, but I had no luck there. Instead, let's consider mixed-precision, using higher precision for computations that need it, and lower precision elsewhere.

*** APEX
    Enter APEX - this library from Nvidia does all the work under-the-hood needed to train a network using mixed-precision operations. In other words, it knows which operations can get away with switching to FP16, and which ones should be done in FP32, and handles the data management accordingly. It's able to do this quite seamlessly by just [[https://en.wikipedia.org/wiki/Monkey_patch][monkey-patching]] over PyTorch functions as needed.

    APEX advertises itself as only needing 3 lines of code to set up. I found there was a slight additional step, in that building it requires a version of CUDA installed that matches the exact version of CUDA used by PyTorch, and my local CUDA was a little out of date. Once I remedied that though, I did indeed just make the changes above.

    By the way, ~O1~ is the recommended/default amount of mixed precision. ~O0~ reverts back to normal FP32, ~O2~ is another mixed precision setting, and ~O3~ is basically FP16.

    After making the above changes and kicking off a new training run, I find memory usage equivalent to FP16. Training for 1 epoch takes a little longer at 170 seconds, and still reaches 0.02 loss. Perhaps the runtime might wash out given a larger/longer training session. Either way though, the 50% extra memory overhead is quite nice, and opens up more possibilities for local training on my own hardware.

** Staff Removal in PyTorch (Revisiting ICDAR 2013)
   :PROPERTIES:
   :EXPORT_FILE_NAME: staff_removal
   :EXPORT_DATE: 2020-02-20
   :END:
   2012 was a significant year for computer vision, as AlexNet smashed past records (and same-year competitors) on the ImageNet recognition challenge. In the following months and years, the field embraced CNN-based techniques, and a vast number of tasks and benchmarks saw major improvements in performance. Because of this, and thanks to the maturity of modern deep learning frameworks, it is quite often the case that pre-deep-learning challenges and benchmarks can be trivially surpassed, often with huge margins, simply by using basic out-of-the-box deep learning techniques.

*** ICDAR Challenge
    Hosted in 2013, the goal of this challenge was to take as input images of sheet music (either binary or grayscale), and then output a binary mask of the sheet music elements, but without the staff lines. Here are some examples (grayscale input, binary input, target result):

    {{{external_image(staff_removal/sample_data.jpg)}}}

    Using grayscale input is clearly a harder problem, given the increased domain and noise. Both types of input are also subject to a variety of noise and geometric distortions, and the handwritten nature of the scores increases variance among samples.

    The training set (and test set) are divided into sections, with each section having varying amounts of degradation (noise and distortion) applied to it, to provide different levels of difficulty on which to evaluate submitted results. See the [[http://www.cvc.uab.es/cvcmuscima/competition2013/][website]] and [[https://hal.archives-ouvertes.fr/hal-00859333/document][published results]] for more details.

    From the [[https://hal.archives-ouvertes.fr/hal-00859333/document][published results]], we see that a variety of heuristics-based techniques were submitted. The top performers have very good F1-scores given binary input, or with low amounts of degradation, but results on grayscale images with higher degradation are not as good, with the best F1-scores a little over 70.

    {{{external_image(staff_removal/submission_scores.jpg)}}}

    As an aside, you may be wondering why staff removal is a useful task at all. In the pre-deep-learning era, many OMR (optical music recognition) systems were built as pipelines of sequential heuristic-based algorithms. Cleaning up the staff as a preprocessing step was useful to simplify downstream steps. Now that end-to-end learning has become more powerful, staff removal as a discrete step will likely fall out of favor (though staves will probably continue to be identified as part of more general segmentation tasks).

*** Preparing Training Data
    Given the unfair advantage of 7 years of deep learning advancement, we're obviously going to try the solve the hardest challenge, with grayscale input and the maximum amount of noise and distortion. After downloading the training data from the website, we'll need to write a data loader class, to load in images and convert them to appropriate tensors.

    Because the images are fairly high-resolution, using them directly is not feasible, at least not with my limited amount of GPU memory. We thus have 2 choices: either downsample the images, or operate on patches of images. Zooming in, we can see that some staff lines are only 1 pixel wide, so downsampling could lose some important data.

    {{{external_image(staff_removal/zoom.jpg)}}}

    Also, identifying staff lines shouldn't require much spatial context - given this 512x512 patch, it's easy to see which pixels correspond to staff lines. In fact, we could likely go much lower than 512x512, though I have not tried.

    {{{external_image(staff_removal/patch.jpg)}}}

    We'll set up our data pipeline to extract patches from images, and classification will be performed one patch at a time. Here's what the data loader code looks like. Note the slightly awkward usage of RandomCrop's parameters passed to functional crop methods. Apparently this is [[https://github.com/pytorch/vision/issues/533][somewhat by design/the recommended way]].

    #+BEGIN_SRC python
    class StaffImageDataset(Dataset):
        def __init__(self, in_files, gt_files, size=(512, 512)):
            self.in_files = in_files
            self.gt_files = gt_files
            self.size = size

        def __getitem__(self, index):
            in_image = Image.open(self.in_files[index])
            gt_image = Image.open(self.gt_files[index])

            y, x, h, w = transforms.RandomCrop.get_params(in_image, output_size=self.size)

            in_image = TF.crop(in_image, y, x, h, w)
            gt_image = TF.crop(gt_image, y, x, h, w)
            return (TF.to_tensor(in_image), TF.to_tensor(gt_image))

        def __len__(self):
            return len(self.in_files)
    #+END_SRC

    It's a little inefficient to load in a large image just to use one small patch - we risk bottlenecking by disk IO, and could instead extract multiple patches at a time. However, I found running DataLoaders in parallel kept my GPU utilization maximized.

    #+BEGIN_SRC python
    in_train, in_test, gt_train, gt_test = train_test_split(in_files, gt_files, test_size=0.1, random_state=0)
    train_dataset = StaffImageDataset(in_train, gt_train)
    test_dataset = StaffImageDataset(in_test, gt_test)
    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=data_loader_parallel)
    test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=data_loader_parallel)
    #+END_SRC

*** Network Choice
    The class of problem we are looking to solve is semantic segmentation, in which every pixel is assigned a label. This a very broadly studied area, with thousands of papers and network architectures. We'll use UNet, which is one of the earlier and simpler architectures, from 2015.

    {{{external_image(staff_removal/unet_architecture.jpg)}}}

    The basic idea, which is now extremely common, is to have a series of contraction layers followed by a series of expansion layers. The contraction layers accumulate spatial information into higher-level features, while the expansion layers spread that higher-level understanding back across pixels. Skip connections are used to preserve high-resolution detail across intermediate levels. Although there are many fantastic open-source implementations available, I decided to implement it myself, just to practice with pytorch and show how easy it is to build up these simpler network architectures.

    #+BEGIN_SRC python
    import torch
    from torch import nn

    # UNet is composed of blocks which consist of 2 conv2ds and ReLUs
    def convBlock(in_channels, out_channels, padding):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=padding),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, 3, padding=padding),
            nn.ReLU()
        )

    # Skip connections are concatenated, cropping if size changed due to no padding
    def cropAndConcat(a, b):
        if (a.shape == b.shape):
            return torch.cat([a, b], 1)

        margin2 = (a.shape[2] - b.shape[2]) // 2
        margin3 = (a.shape[3] - b.shape[3]) // 2
        a_cropped = a[:, :, margin2 : margin2 + b.shape[2], margin3 : margin3 + b.shape[3]]
        return torch.cat([a_cropped, b], 1)

    class UNet(nn.Module):

        # Depth includes the bottleneck block. So total number of blocks is depth * 2 - 1
        # Unexpected output sizes or num channels can occur if parameters aren't nice
        # powers of 2
        def __init__(self,
                     input_channels=1,
                     output_channels=2,
                     depth=5,
                     num_initial_channels=64,
                     conv_padding=0
                     ):
            super().__init__()

            # Going down, each conv block doubles in number of feature channels
            self.down_convs = nn.ModuleList()
            in_channels = input_channels
            out_channels = num_initial_channels
            for _ in range(depth-1):
                self.down_convs.append(convBlock(in_channels, out_channels, conv_padding))
                in_channels = out_channels
                out_channels *= 2

            self.bottleneck = convBlock(in_channels, out_channels, conv_padding)

            # On the way back up, feature channels decreases.
            # We also have transpose convolutions for upsampling
            self.up_convs = nn.ModuleList()
            self.tp_convs = nn.ModuleList()
            in_channels = out_channels
            out_channels = in_channels // 2
            for _ in range(depth-1):
                self.up_convs.append(convBlock(in_channels, out_channels, conv_padding))
                self.tp_convs.append(nn.ConvTranspose2d(in_channels, out_channels,
                                                        kernel_size=2, stride=2))
                in_channels = out_channels
                out_channels //= 2

            # final layer is 1x1 convolution, don't need padding here
            self.final_conv = nn.Conv2d(in_channels, output_channels, 1)

            # max pooling gets applied in a couple places. It has no
            # trainable parameters, so we just make one module and reuse it.
            self.max_pool = nn.MaxPool2d(2)

        def forward(self, x):
            features = []
            for down_conv in self.down_convs:
                features.append(down_conv(x))
                x = self.max_pool(features[-1])

            x = self.bottleneck(x)

            for up_conv, tp_conv, feature in zip(self.up_convs, self.tp_convs, reversed(features)):
                x = up_conv(cropAndConcat(feature, tp_conv(x)))

            return self.final_conv(x)
    #+END_SRC

    The 3 main parameter choices are number of layers, initial number of feature channels, and type of padding. I initially tried 5 layers, 64 features, valid padding, as is used in the paper. The number of parameters took up a lot of my gpu memory though, and training was quite slow. I switched to 3 layers and 32 features, which drastically reduced memory usage and sped up training time. It's likely network size could be reduced more without much effect on performance (after all UNet has been used to solve much harder problems than this), but I did not test further. I also switched from valid padding to zero padding, which means border pixels are influenced by "fake" values. This is often argued to perform worse, but it makes the data handling a bit simpler, as output sizes match input sizes.

*** Training
    With a data loader and a network, all that's left is to train. We simply pick an optimizer and loss function (both just arbitrary default-ish choices), and put together a basic training loop. I use apex.amp to support larger batch sizes on my local GPU.

    #+BEGIN_SRC python
    epochs=10
    learning_rate=0.001

    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    net = UNet(depth=3, num_initial_channels=32, conv_padding=1).to(device)

    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)

    net, optimizer = amp.initialize(net, optimizer, opt_level="O1")

    # The training loop
    total_steps = len(train_data_loader)
    for epoch in range(epochs):
        net.train()
        for i, (in_images, gt_images) in enumerate(train_data_loader, 1):
            preds = net(in_images.to(device))
            gt_images = gt_images.squeeze(1).type(torch.LongTensor).to(device)
            loss = criterion(preds, gt_images)

            optimizer.zero_grad()
            with amp.scale_loss(loss, optimizer) as scaled_loss:
                scaled_loss.backward()
            optimizer.step()

            if (i) % 10 == 0:
                print (f"Epoch [{epoch + 1}/{epochs}], Step [{i}/{total_steps}], Loss: {loss.item():4f}")

        # Save after each epoch
        torch.save({'epoch': epoch,
                    'model_state_dict': net.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': loss
        }, 'checkpoint' + str(epoch) + '.ckpt')

        # Evaluate validation after each epoch
        net.eval()
        with torch.no_grad():
            sum_loss = 0
            for in_images, gt_images in test_data_loader:
                preds = net(in_images.to(device))
                gt_images = gt_images.squeeze(1).type(torch.LongTensor).to(device)
                sum_loss += criterion(preds, gt_images)
            print(f'validation loss: {(sum_loss / len(test_data_loader)):4f}')
        #+END_SRC

*** Results
    With this basic network and training setup, each epoch took around 2 minutes to train for me, and validation loss flattened out after 5 epochs, for a total training time of 10 minutes. Note that these training images are around 8 megapixels, and I only sampled 512x512 patches from them. That means my overall training run only looked at around 15% of available pixels before saturating.

    With our binary-patch-semantic-segmentation network trained, we can now classify each patch in each image in the test set. Note that we would likely get best results by overlapping patches and combining their predictions, but I simply used adjacent patches, overlapping as needed at the borders to fit irregular image dimensions.

    Here are 2 inputs, followed by predictions and ground truths, where the first case is an "easy" sample, and the second has more noise. Interestingly, point noise as visible in the bottom sample is kept in the ground truth output, and our network learned to do the same. Our network is fooled by the crease in the lower-left corner though.

    {{{external_image(staff_removal/result_examples.jpg)}}}

    After running inference on the test set, we can compute our score using the test ground truth published after the competition. Recall that the top submissions in 2013 reached an F1 score around *0.72*. With our basic UNet and 10 minutes of training, we obtain an F1 score of *0.966* across all 2000 test images. Looking at just the 1000 test images with the highest levels of degradation, F1 score only drops to *0.959*.

    This is really no surprise considering the much more complex problems being tackled these days, but it's nice to look at what can be solved with just the bare minimum of today's techniques.

** Email Bomb
   :PROPERTIES:
   :EXPORT_FILE_NAME: email_bomb
   :EXPORT_DATE: 2019-09-29
   :END:
   {{{other_repo_image(mailBombAnalysis/master/email_rate.jpg)}}}
   On August 12, for about 24 hours my email inbox was flooded with emails, peaking at over 1 email/second. This type of attack is known as an [[https://en.wikipedia.org/wiki/Email_bomb][email bomb]], and the intent is to overwhelm email providers and/or user attention as cover for other simultaneous attacks (which might send emails from password changes, online purchases, etc.).

The attacker did not use their own computing resources to send emails - instead, the attacker had a list of mailing lists, and used a script to subscribe my email address to each one. Each mailing list then sent me a welcome email. This makes email bombs difficult to prevent, as there's no single source to block, and furthermore many of these mailing lists belong to legitimate businesses.

Although the attack occurred many weeks ago, I'm still an unwitting member of these countless email lists, and have received a steady stream of unwanted daily newsletters, promotional offers, blog posts, etc. Most of them do go to the spam folder, but that still means any attempt to search for legitimate emails in my spam folder is difficult. So to address this, I'm going to write some code to click on all the unsubscribe links in emails in my spam folder.

*** Downloading Emails
    Email data can be easily downloaded via the gmail api. I'll be using the Python version. The first step is to get a gmail api service object, which can just be copied from the [[https://developers.google.com/gmail/api/quickstart/python][quickstart tutorial code]] and will likely end in something like this.
    #+BEGIN_SRC python
    service = build('gmail', 'v1', credentials=creds)
    #+END_SRC

    Next let's figure out the email label id corresponding to the spam folder.
    #+BEGIN_SRC python
    labels = service.users().labels().list(userId='me').execute().get('labels', [])
    spam_label_id = next(label['id'] for label in labels if label['name'] == 'SPAM')
    #+END_SRC

    Each email is referenced by a message object, and we can only request a page of messages at a time. Each response provides the necessary information to request the next page, so we use a loop to accumulate up the messages.
    #+BEGIN_SRC python
    def getMessagesWithLabels(service, user_id, label_ids):
        response = service.users().messages().list(userId=user_id,
                                                   labelIds=label_ids).execute()
        messages = []
        if 'messages' in response:
            messages.extend(response['messages'])
            while 'nextPageToken' in response and not DEBUG:
                print('\rFound %d messages' % len(messages), end='') # carriage return to overwrite
                page_token = response['nextPageToken']
                response = service.users().messages().list(userId=user_id,
                                                           labelIds=label_ids,
                                                           pageToken=page_token).execute()
                messages.extend(response['messages'])
        print() # new line after carriage returns
        return messages
    #+END_SRC
    #+BEGIN_SRC python
    min_messages = getMessagesWithLabels(service, 'me', [spam_label_id])
    #+END_SRC

    These message objects only contain identifiers - getting any actual email information requires making further queries using those ids. Before downloading the full message bodies, let's first try grabbing some basic metadata.
    #+BEGIN_SRC python
    # The data we will gather
    data = [['epoch_ms', 'from', 'reply-to', 'subject']]

    # The callback for each message
    def getMsgData(rid, message, exception):
        if exception is not None:
            return
        epoch_ms = int(message['internalDate'])
        fromx = ''
        reply_to = ''
        subject = ''
        headers = message['payload']['headers']
        for h in headers:
            if h['name'] == 'From':
                fromx = h['value']
            elif h['name'] == 'Reply-To':
                reply_to = h['value']
            elif h['name'] == 'Subject':
                subject = h['value']
        data.append([epoch_ms, fromx, reply_to, subject])

    # Batching requests is faster
    batcher = service.new_batch_http_request()
    for i, mm in enumerate(min_messages):
        if (i % 100 == 0 and i != 0):
            print(f'\rRequesting msg {i}', end='')
            batcher.execute()
            batcher = service.new_batch_http_request()
        batcher.add(service.users().messages().get(userId='me', id=mm['id'], format='metadata'), callback=getMsgData)
    print() # new line after carriage returns
    # Handle last set
    batcher.execute()

    with open('data.csv', 'w') as f:
        writer = csv.writer(f)
        writer.writerows(data)
    #+END_SRC


*** Email Rate
    With this data in hand, we can make some plots. Here's the rate of emails/hour over the entire month. Note the logarithmic y-axis.
    {{{other_repo_image(mailBombAnalysis/master/email_rate.jpg)}}}
    Before August 12, I rarely received emails to the spam folder, and never more than 1/hour. Then, a surge of emails, reaching 3719 spam emails per hour at its peak. Afterwards, there's a regular pattern to the email frequency, still far above the initial rate.
    {{{other_repo_image(mailBombAnalysis/master/email_rate2.jpg)}}}
    The rate and pattern hold fairly steady through September.

*** Who are the Offenders?
    The emails I received on August 12 were mostly welcome emails. Because the volume of emails I receive now is significantly lower, it can be assumed that most mailing lists required subscription confirmation. Let's see who is sending emails without subscription confirmation, based on September data. This isn't that surprising, as the vast majority of lists are sending emails roughly once per weekday.
    {{{other_repo_image(mailBombAnalysis/master/domains.jpg)}}}
    {{{other_repo_image(mailBombAnalysis/master/names.jpg)}}}
    Also unsurprising, there's not much variation in the email names, though I guess Steve and Holly are the most likely names for email marketers.

*** Automated Unsubscription
    To actually unsubscribe, we'll need to download each email's contents, search for the unsubscribe link, and click on it. To get the full message body we need to update the message request to ~format='full'~.
    #+BEGIN_SRC python
    batcher.add(service.users().messages().get(userId='me', id=mm['id'], format='full'), callback=getMsgData)
    #+END_SRC
    The message contents as an html string can be obtained as follows
    #+BEGIN_SRC python
    def getMsgData(rid, message, exception):
        if exception is not None:
            return
        try:
            msg = next(m for m in message['payload']['parts'] if m['mimeType'] == 'text/html')
        except:
            return
        msg_data = msg['body']['data']
        msg_html = base64.urlsafe_b64decode(msg_data.encode('ASCII')).decode('utf-8')
    #+END_SRC
    We'll cast a wide net by collecting any and all links that contain "unsubscribe" in their text. Python's built-in html parser steps through tags and the data between tags, so we can use it to extract all links fitting our criteria.
    #+BEGIN_SRC python
    class UnsubLinkParser(HTMLParser):
        a_href = ''
        unsub_links = []

        def handle_starttag(self, tag, attrs):
            if tag == 'a':
                for attr in attrs:
                    if attr[0] == 'href':
                        self.a_href = attr[1]
                        break

        def handle_endtag(self, tag):
            if tag == 'a':
                self.a_href = ''

        def handle_data(self, data):
            if self.a_href != '' and 'unsubscribe' in data.lower():
                self.unsub_links.append(self.a_href)
                self.a_href = ''
    #+END_SRC

    With our links gathered up, we can simply visit each one in turn:
    #+BEGIN_SRC python
    for link in parser.unsub_links:
        urllib.request.urlopen(link)
    #+END_SRC
    Some unsubscribe links will require further action, such as clicking a 'submit' button. For now let's ignore that, and see how effective this simple method is.

    *Edit from a week later*: There's been a decrease of around 10% or so. There seems to be 3 reasons for this. 1: Some sites require more than just the single button click. 2: Many messages occur less than once a month, meaning they weren't in the spam folder (which gets auto-cleaned every 30 days) at the time of running my script. Running every week or so has continued to slowly decrease the email rate. 3: At least half of the current spam emails are not in english, meaning I need to compile a list of "unsubscribe" in other languages

** Graph Cuts on Markov Random Fields
   :PROPERTIES:
   :EXPORT_FILE_NAME: graph_cuts
   :EXPORT_DATE: 2019-07-09
   :END:
   |            | Binary                                                                                                 | Multi-label                                                                                     |
   |------------+--------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------|
   | Submodular | Exact polynomial-time solution via min-cut/max-flow                                                    | Exact polynomial-time solution via min-cut/max-flow                                             |
   | Metric     | N/A                                                                                                    | NP-hard, polynomial-time *alpha-expansion* reaches local-min within a factor of 2 of global min |
   | Neither    | NP-hard, polynomial-time *quadratic pseudo-boolean optimization* can produce an exact partial solution | NP-hard, polynomial-time *alpha-beta swap* reaches local-min                                    |

*** Submodularity
   Binary submodular cost functions satisfy:
   #+BEGIN_SRC python
   Cost(a,b) + Cost(b,a) - Cost(a,a) - Cost(b,b) >= 0
   #+END_SRC
   Multi-label submodular cost functions satisfy:
   #+BEGIN_SRC python
   Cost(b,c) + Cost(a,d) - Cost(b,d) - Cost(a,c) >= 0, where b > a and d > c
   #+END_SRC
   From a set theory perspective, a cost function is submodular if adding an element x to set S incurs a cost increase \alpha, which is less than or equal to the cost increase \beta incurred by adding element x to set T, where T is any subset of S. In other words, submodularity implies a diminishing-costs effect.

   Convex cost functions (where smoothness is preferred and larger label differences have larger costs) are a common class of submodular costs.

*** Metric costs
   Metric cost functions satisfy the following criteria:
   #+BEGIN_SRC python
   Cost(a,a) = 0
   Cost(a,b) > 0
   Cost(a,b) = Cost(b,a)
   Cost(a,c) <= Cost(a,b) + Cost(b,c)
   #+END_SRC

** Serial Access for R8000/AC3200 (and other) Routers
   :PROPERTIES:
   :EXPORT_FILE_NAME: serial_router
   :EXPORT_DATE: 2019-07-04
   :END:
   {{{external_image(serial_router/router5.jpg)}}}
   So you bricked your router. Or maybe you just want a more convenient way to manage and monitor firmware upgrades (wiping settings via command is a lot more pleasant than holding down power buttons). Either way, adding serial access is pretty easy for many routers. I first did this a couple years ago, but I had to do it again recently, so I documented the process here for my current router (Netgear R8000/AC3200). I've also since discovered that there are pretty good instructions on the [[https://wiki.dd-wrt.com/wiki/index.php/Serial_Recovery][dd-wrt wiki]] and [[https://www.myopenrouter.com/article/how-set-serial-console-netgear-r8000][myopenrouter]] as well.
*** Tools
    The main thing you need is a setup that has usb on one end (for the computer), and standard serial pins (at least RX, TX, ground) on the other end. *Important:* the serial side needs to be at 3.3v, and usb operates at 5v, so make sure you have a level shifter in there somewhere. I believe there's some cables that have this all in one package, but I ended up using [[https://smile.amazon.com/OSEPP-Breakout-Board-Arduino-Compatible/dp/B007JBSSGQ][this breakout board]] which I purchased from Fry's. Anything that mentions USB to TTL, and 3.3V should work fine though. If you use a board like this you'll also need some wires and possibly a soldering iron (though tape or extra hands work just fine for a temporary unbricking setup).
*** Getting to the pins (R8000 specific)
    1. Remove the torx screws on the bottom and back, including {{{external_link(serial_router/router1.jpg, the one hidden under the bottom label)}}} (no turning back after the label is broken, if you care about warranty!)
    2. Flip the router over, remove the bottom cover, and detach the antennas (6 colored wires), which {{{external_link(serial_router/router2.jpg, should look something like this)}}}.
    3. There's still a ribbon cable attaching the main board to the rest of the router, but it's long enough that the board can be flipped over without disconnecting it, {{{external_link(serial_router/router3.jpg, like this)}}}. The serial pins are now accessible (top left in the prior image).
*** Pin layout
    {{{external_image(serial_router/router4.jpg)}}}
    In the above image, the pin with the red wire attached is RX, orange is TX, and yellow is ground. The 4th pin is not needed here. If you are using a breakout board like me, keep in mind that RX on the router should go to TX on the board, and vice versa. The image at the top of this post shows my final setup, complete with drilled hole for semi-permanent access (note the red and orange wires swapping near the breakout board).
*** Computer stuff
    On the computer end, any serial program like PuTTY or minicom will work. Find and select the usb device via something like device manager or dmesg, set baud rate to 115200, and everything else should pretty much be defaults. With everything connected, you should be able to see a stream of text output whenever the router boots. If you don't, you can verify your setup by disconnecting from the router and shorting between RX and TX, and making sure any typed text is echoed back.
*** Commands
    To get to a command prompt, reboot the router and mash ~Ctrl-C~ a bunch as it starts up.
    Pretty much the only command I use is ~nvram erase~, which resets router settings, and has generally resolved any boot issues I've encountered. You can also apply and transfer new firmware over telnet for more serious problems, and do a whole bunch of other things, but I'll leave those for other sites to cover, at least until I cause more problems and need to figure those things out for myself.

** Hungarian Matching Demo
   :PROPERTIES:
   :EXPORT_FILE_NAME: hungarian_matching
   :EXPORT_DATE: 2019-07-03
   :END:
   Back in 2013, as a class project, we built a javascript demo of the hungarian algorithm. The basic idea is that it's a polynomial-time method to obtain the optimal matching between 2 sets of objects (e.g. matching people to resources), where every pairing has some cost (or reward) associated with it. I had never used javascript before this project, and I never used it again afterwards, so no idea if the code itself is any good, but it was a fun project.

<iframe width=1000 height=700 src=../../files/graphVisualizer/graphVisualizer.html></iframe>

** Building Meshlab from Source in Ubuntu
   :PROPERTIES:
   :EXPORT_FILE_NAME: meshlab-build
   :EXPORT_DATE: 2018-06-16
   :END:
   Every time I build Meshlab, it's always a little more work than it really should be. So here's my notes from my most recent build (June 2018, Ubuntu 18.04)

   Clone the repositories (This is for building master, switch to a release branch/tag if you prefer)
   #+BEGIN_SRC sh
   git clone git@github.com:cnr-isti-vclab/meshlab.git
   git clone git@github.com:cnr-isti-vclab/vcglib.git -b devel
   #+END_SRC
   Install dependencies (You may need other dependencies, these are just the ones that I needed at this point in time)
   #+BEGIN_SRC sh
   sudo apt install qt5-qmake qtscript5-dev libqt5xmlpatterns5-dev libqt5widgets5 libqt5gui5 libqt5network5 libqt5core5a libdouble-conversion1 libxcb-xinerama0
   #+END_SRC
   Build external plugins
   #+BEGIN_SRC sh
   cd meshlab/src/external
   qmake -qt=5 external.pro
   make -j6
   #+END_SRC
   Build common project
   #+BEGIN_SRC sh
   cd ../common
   qmake -qt=5 common.pro
   make -j6
   #+END_SRC
   At this point I encountered an error about =ReadHeader=. The following GitHub issue contains a fix, and I've pasted the patch below
   https://github.com/cnr-isti-vclab/meshlab/issues/188
   #+BEGIN_SRC diff
   diff -ru vcglib/wrap/io_trimesh/import_nvm.h vcglib/wrap/io_trimesh/import_nvm.h
   --- a/vcglib/wrap/io_trimesh/import_nvm.h	2016-12-29 12:54:58.000000000 +0300
   +++ b/vcglib/wrap/io_trimesh/import_nvm.h	2017-12-28 12:20:14.591670159 +0300
   @@ -85,15 +85,6 @@
   return true;
   }

   -static bool ReadHeader(const char * filename,unsigned int &/*num_cams*/, unsigned int &/*num_points*/){
   -    FILE *fp = fopen(filename, "r");
   -    if(!fp) return false;
   -    ReadHeader(fp);
   -    fclose(fp);
   -    return true;
   -}
   -
   -
   static int Open( OpenMeshType &m, std::vector<Shot<ScalarType> >  & shots,
   std::vector<std::string > & image_filenames,
   const char * filename, CallBackPos *cb=0)
   diff -ru vcglib/wrap/io_trimesh/import_out.h vcglib/wrap/io_trimesh/import_out.h
   --- a/vcglib/wrap/io_trimesh/import_out.h	2016-12-29 12:54:58.000000000 +0300
   +++ b/vcglib/wrap/io_trimesh/import_out.h	2017-12-28 12:20:48.434017234 +0300
   @@ -85,15 +85,6 @@
   return true;
   }

   -static bool ReadHeader(const char * filename,unsigned int &/*num_cams*/, unsigned int &/*num_points*/){
   -    FILE *fp = fopen(filename, "r");
   -    if(!fp) return false;
   -    ReadHeader(fp);
   -    fclose(fp);
   -    return true;
   -}
   -
   -
   static int Open( OpenMeshType &m, std::vector<Shot<ScalarType> >  & shots,
   std::vector<std::string > & image_filenames,
   const char * filename,const char * filename_images, CallBackPos *cb=0)
   #+END_SRC
   Build meshlab itself
   #+BEGIN_SRC sh
   cd ..
   qmake -qt=5 meshlab_full.pro
   make -j6
   #+END_SRC
   I hit an error about missing libraries - the following fixed it for me
   #+BEGIN_SRC sh
   cp external/lib/linux/* external/lib/linux-g++
   #+END_SRC
   If everything worked, the meshlab binary will be at src/distrib/meshlab
** Dual-booting Ubuntu 18.04 with macOS (including full disk encryption)
   :PROPERTIES:
   :EXPORT_FILE_NAME: ubuntu-on-macbook-pro
   :EXPORT_DATE: 2018-06-08
   :END:
*** Introduction
     I've been running Ubuntu on Macbook Pros for a couple years now, and while the ease of installation, driver support, and general stability has greatly improved in recent years, it can be difficult to find up-to-date guides. I've recently set up a mid-2015 macbook pro dual booting macOS with Ubuntu 18.04, so I figured I'd document my steps. First some overall notes and warnings, then simple instructions for a non-encrypted install, followed by slightly longer instructions for an encrypted install.

*** Notes and Warnings
     * I've heard that support for the newer touchbar-equipped macbook pros is not great. I have not tried those, but I've used a mid-2014, as well as 2 variants of mid-2015 macbook pros long-term, on 14.04, 16.04, and 18.04.
     * If you get your disk into any terrible state, macbooks come with pretty great recovery options. Command-R will boot into a recovery partition, and even if that's lost, Option-R will get you into an internet-recovery mode.
     * If you want to remove Ubuntu, and find that grub is still hanging around (or somehow end up with an extraneous grub), run the following from macOS.
       #+BEGIN_SRC sh
       mkdir mnt
       sudo mount -t msdos /dev/disk0s1 mnt
       sudo rm -rf mnt/EFI/ubuntu
       #+END_SRC
     * System upgrades of either macOS or ubuntu may cause refind to lose priority and make it more difficult to dual-boot. If that happens, you can run refind-mkdefault, which is available in the mac download as explained below, or from ~sudo apt install refind~. For more information see this [[https://www.rodsbooks.com/refind/bootcoup.html][handy guide]] from the refind website.

*** Create Ubuntu bootable USB
     Instructions for [[https://tutorials.ubuntu.com/tutorial/tutorial-create-a-usb-stick-on-macos#0][macOS]], [[https://tutorials.ubuntu.com/tutorial/tutorial-create-a-usb-stick-on-ubuntu#0][Ubuntu]], [[https://tutorials.ubuntu.com/tutorial/tutorial-create-a-usb-stick-on-windows#0][Windows]]

*** Prepare macOS
     First thing we'll need to do is reduce your macOS partition size in order to make some space for Ubuntu. This should be fairly straightforward using macOS's Disk Utility applicaton.

     Next, install rEFInd, which is available [[http://www.rodsbooks.com/refind][here]], and run the refind-install binary. Most likely you'll see an error message about System Integrity Protection being enabled. As the error message suggests, we can either install from the recovery partition, or temporarily disable SIP. To get into recovery mode, hold command + r while booting, and from there a terminal can be accessed via the Utilities menu. You can try running refind-install from recovery mode, but I had no luck with that, and just got the same error. So instead, I ran =csrutil disable= to disable SIP. After a reboot (back to non-recovery mode, because it's faster), refind-install should work. You can then re-enter recovery mode to run =csrutil enable=. After this process, you should now see the refind menu whenever you boot. You'll be able to choose between macOS and any other operating systems you load, as well boot from external drives.

*** Install Ubuntu 18.04 (no encryption)
     After booting from the Ubuntu bootable USB, you can either install straightaway, or do it from within the "try ubuntu" environment. Either way, the only important step is to select "Something else" on the menu that asks how/where to install Ubuntu. You should see the empty space on your disk that you freed up from macOS, and be able to add partitions. This is my configuration:
     * Boot partition, 500 MB, ext4, mounted at /boot (sda4 for me)
     * Root partition, remaining space, ext4, mounted at / (sda5 for me)
     * Bootloader installed to boot partition (sda4 for me)
     You could optionally add a swap partition, but Ubuntu 18.04 now supports swap files

     Everything should be good to go from here. As a side-note the installer crashed for me apparently because I had another copy of grub hanging out on my /sda from some earlier tests. Deleting it per the notes above, and then retrying worked for me.

*** Install Ubuntu 18.04 with full disk encryption
     After going through the above process, I discovered that Ubuntu 18.04 no longer supports homedir encryption. Furthermore, while full disk encryption is an option in the installer, it requires wiping the entire physical disk. So that's not great either. Fortunately, I came across this well-written [[https://blog.jayway.com/2015/11/22/ubuntu-full-disk-encrypted-macosx/][blog post]] that provides all the details to manually encrypt the Ubuntu partition before installing. As above, I chose to skip the swap partition steps, and otherwise followed it with only one issue.

     Strangely, I again had the installer repeatedly crash on me while "copying files". This time it was not due to any grub conflicts that I could find. As an unsatisfying workaround, I realized that it would only crash after I entered my account/login details. So I simply stayed on that screen until the activity led on my usb drive stopped flashing. I then continued forward, allowed it to crash, and then moved on with the post-installation instructions, and so far things are working.
** Publishing a Website from Emacs and Hugo
   :PROPERTIES:
   :EXPORT_FILE_NAME: website-v2-setup
   :EXPORT_DATE: 2018-06-04
   :END:
*** Introduction
   After 5 years, it's time to give the site a bit of a refresh, now with fewer images and more words. Previously I used bootstrap plus a bit of manual editing. This time I'll be using a pipeline of Emacs org-mode -> ox-hugo -> hugo -> nearlyfreespeech.net. This post will self-document my steps to get all that up and running. The last time I did any web-related things was over 5 years ago, and I wasn't an expert then, so these steps should be taken with a grain of salt.
*** Hugo Setup
    #+BEGIN_SRC sh
    sudo snap install hugo
    mkdir petercheng && cd petercheng
    hugo new site petercheng
    #+END_SRC
    Emacs init:
    #+BEGIN_SRC lisp
    (use-package ox-hugo
        :ensure t
        :after ox)
    #+END_SRC
    Set up a theme (I'm using the [[https://themes.gohugo.io/hyde-hyde/][hyde-hyde]] theme)
    #+BEGIN_SRC sh
    git submodule add https://github.com/htr3n/hyde-hyde.git themes/hyde-hyde
    #+END_SRC
*** config.toml
    For my intended setup, there are only 2 files I'll be working with. The first one is ~config.toml~, which stores global hugo settings, as well as parameters for my chosen theme. I'm not really sure how to find all the toggle-able parameters for a given theme besides digging through the theme code or looking at example sites.

    As an early example of why I'm using org-mode, I can directly insert a live copy of my ~config.toml~ file below, simply by including the line:

    ~#+INCLUDE: "config.toml" src ini~
    #+INCLUDE: "config.toml" src ini

    One early roadblock I hit was that hyde-hyde uses highlight.js for syntax highlighting, which does not contain ~emacs-lisp~ as a language option, unlike org-mode and chroma (hugo's default syntax highlighter). I'm currently using ~lisp~ as a compromise, and it took me a while to realize that highlightjslanguages needed to be set to include non-default languages in highlight.js. If an unsupported (or empty!) language is passed to highlight.js, at least with hyde-hyde, it results in poorly formatted output, which led to much confusion for a while.

*** petercheng.org
    The other file I need to create is the org file that generates all this content, on every page, following ox-hugo's single-page architecture. In normal Hugo, individual pages written in markdown (or now in org-mode) are placed inside the ~content~ directory inside the project root. With ox-hugo, a single org-mode file can be used to generate all pages, posts, and any other content. This has some advantages in allowing usage of org-mode functionality, as well as re-use of content or property settings across pages.

    There's a number of hugo properties that can be set within the file, but the only required one is ~HUGO_BASE_DIR~, which specifies the root directory of the hugo website, relative to the org file.
    #+BEGIN_SRC sh
    #+HUGO_BASE_DIR: ./
    #+END_SRC

    Afterwards, I have 2 top-level sections in my org file, ~Pages~, and ~Posts~. Any properties set under a section will be applied to subsections, so I have the following properties set for each, to place pages at the top level of my exported files, and posts within a subdirectory.
    #+BEGIN_SRC sh
    * Pages
        :PROPERTIES:
        :EXPORT_HUGO_SECTION: ./
        :END:
    * Posts
        :PROPERTIES:
        :EXPORT_HUGO_SECTION: posts
        :END:
    #+END_SRC
    I can then create pages or posts by creating subsections within the relevant section. The ~EXPORT_FILE_NAME~ property is required to be set for each, which determines the exported filename. Here's an example of the properties setting for this current post.
    #+BEGIN_SRC markdown
    ** Publishing a Website from Emacs and Hugo
        :PROPERTIES:
        :EXPORT_FILE_NAME: website-v2-setup
        :EXPORT_DATE: 2018-06-04
        :END:
    #+END_SRC

*** Exporting
    Ox-hugo adds a new export option to the org-mode export menu. ~(C-c C-e)~ by default. There's a few options for exporting, but currently I find it simplest just to always export all content, with ~(C-c C-e H A)~. One setting I've seen used a lot is ~#+HUGO_AUTO_SET_LASTMOD: t~, and that doesn't play nicely if always updating all files. But I don't feel a need to track and update dates on every edit.

    After exporting, markdown files should be created in the content directory, and hugo will auto-reload pages if already running (to start hugo, run ~hugo server~ from the base directory).

*** Getting Online
    There are some fancy options for deploying, such as [[https://www.penwatch.net/cms/get_started_plain_blog/][this guide]], which demonstrates hugo publishing on a remote server, triggered by git post-receive. For the time being I'm going to keep thing simple, and simply use a script to generate a static site, which I'll keep synced up via rsync. A final example of showing a live code view of my publishing script:
    #+INCLUDE: "publish.sh" src bash


